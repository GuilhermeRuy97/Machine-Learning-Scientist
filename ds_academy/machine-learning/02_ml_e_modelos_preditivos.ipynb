{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dados precisam ser lapidados para que sirvam de algum usufruto.\n",
        "\n",
        "A área de BI, por exemplo, realiza **modelos descritivos** dos dados, analisando-se o passado. <br>\n",
        "Com o machine learning, tentamos realizar predições para o futuro, encontrando a função matemática aproximada que pode realizar essa predição.<br>\n",
        "Em um modelo de regressão linear por exemplo (Y = a+bx), encontra-se os valores de 'a' e 'b' que se adequem aos dados.\n",
        "\n",
        "Vale lembrar que para os métodos preditivos, existem os métodos baseados em **instância**, **probabilísticos**, baseados em **procura**, baseados em **otimização** etc. Essa escolha pode afetar o tempo de treinamento, métricas de avaliação, complexidade do modelo etc.\n",
        "\n",
        "Além de escolhermos o melhor algoritmo e método, devemos nos atentar também a escolha das variáveis utilizadas (**feature selection**, redução de dimensionalidade etc).\n",
        "\n",
        "Lembre-se sempre que ***o negócio é mais importante que a tecnologia empregada***. Identifique sempre as melhores e mais precisas perguntas a se fazer para que se obtenha as respostas adequadas.\n",
        "\n",
        "No exemplo do dataset 'Iris', os dados sobre as pétalas representam as variáveis preditoras, e a espécie representa a variável target (ou alvo ou label). Ou seja, neste 'negócio', buscamos desvendar a espécie de uma planta dadas algumas características dela.\n",
        "\n",
        "---\n",
        "\n",
        "<h3>Métodos</h3>\n",
        "\n",
        "Baseado em **Instância**: <br>\n",
        "Armazena os exemplos de treinamento e generaliza quando uma nova instância for classificada, calculando a distância entre o novo ponto de dado e os exemplos armazenados, retornando aquele que tem a menor distância.\n",
        "A função que determina a similaridade entre 2 pontos de dados é a distância Euclidiana ou distância Manhattan para atributos contínuos e [0 ou 1] para atributos categóricos.\n",
        "* OBS: tem-se que realizar a normalização dos dados para o intervalo [0,1] em atributos contínuos.\n",
        "* Outras medidas de distância são a Correlação de Pearson, Similaridade de Cosseno (usado em classificação de textos e dados de alta dimensão) e Distância de Edição (ex. distância entre strings em classificação de textos).\n",
        "\n",
        "Ex.\n",
        "Podemos tentar prever o valor de uma casa dado o número de janelas nela. <br>\n",
        "O algoritmo irá calcular a distância entre a nova casa e as antigas armazenadas, retornando o valor mais próximo encontrado.\n",
        "\n",
        "- Uma vantagem deste método é de se não ser paramétrico (podem ser usadas distribuições arbitrárias sem o conhecimento da densidade).\n",
        "- Uma desvantagem deste método é o custo de classificação, já que **todo o processamento ocorre no momento de classificação** (ao se comparar o novo dado com os antigos), ou seja, não é construído um modelo de classificação explícito.\n",
        "- O treinamento é fácil, já que se trata de uma \"memorização\" de exemplos.\n",
        "- A etapa de teste exige um alto custo, já que também compara os novos valores com todos os exemplos de treino.\n",
        "\n",
        "Ex. de algoritmo desta abordagem:\n",
        "- KNN (K-Nearest Neighbours)\n",
        " - Atribui um rótulo representado mais frequentemente entre os exemplos de amostras (K) mais próximas.<br>\n",
        "<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final_a1mrv9.png\">\n",
        "\n",
        "---\n",
        "\n",
        "Métodos **Probabilísticos**: <br>\n",
        "Os métodos probabilísticos Bayesianos nos dizem que a probabilidade de A ocorrer, dado um evento B, não depende apenas da relação de A e B, mas também da probilidade de A ocorrer independentemente de B.<br>\n",
        "<img src=\"https://miro.medium.com/max/1000/0*6Cp7n-J0DVRPFqAz.jpg\"><br>\n",
        "Neste caso, também se trata de um algoritmo matemático.\n",
        "\n",
        "Ex de algoritmos\n",
        " - Naive Bayes\n",
        " - Redes Bayesianas\n",
        "\n",
        "Cada exemplo de treino pode incrementar ou decrementar a probabilidade de uma hipótese estar correta. Por exemplo, um dado cliente de banco tem uma chance de x% de ser um bom pagador.\n",
        "\n",
        "Ex.<br>\n",
        "P(Bom pagador = presente) = 0.85<br>\n",
        "P(Bom pagador = ausente) = 0.15<br>\n",
        "P(Casado = positivo | Bom pagador = presente) = 85%<br>\n",
        "P(Casado = falso | Bom pagador = ausente) = 20%\n",
        "\n",
        "No caso do Naive Bayes:<br>\n",
        "Posterior = ( prior x likelihood ) / evidence<br>\n",
        "ou<br>\n",
        "P(A|B) = ( P(B|A) * P(A) ) / P(B) <br>\n",
        "Ou seja, os valores dos atributos são condicionalmente independentes dado o valor alvo.\n",
        "\n",
        "---\n",
        "\n",
        "Aprendizagem baseada em **Procura**: <br>\n",
        "Um exemplo desta aprendizagem é a árvore de decisão  (decisão para classificação e árvore de regressão para regressão), muito útil em mineração de dados, laudos médicos etc (ex. akinator com if's e else's). <br>\n",
        "É um bom modelo para dados com ruídos.<br>\n",
        "É também uma boa estrutura para compreensão do modelo.<br>\n",
        "Outros exemplos famosos são o ID3 (testa-se a capacidade dos atributos de se tornarem o nó raíz, através da medida de entropia [medindo a certeza sobre um evento] e a medida de ganho de informação [o quanto uma variável é boa para explicar os dados por si só]) e C4.5.<br>\n",
        "<img src=\"https://didatica.tech/wp-content/uploads/2020/07/image-5.png\">\n",
        "\n",
        "Logo, o método de procura pode ser utilizado também para se procurar as melhores features de um conjunto de dado.\n",
        "\n",
        "---\n",
        "\n",
        "Aprendizagem baseada em **Otimização**: <br>\n",
        "\n",
        "Entre os exemplos mais famosos da aprendizagem por otimização são as redes neurais artificiais e o Support Vector Machines (SVM) (separa objetos através de uma função de otimização quadrática), que envolvem uma regra de otimizar uma função que corrige erros.\n",
        "\n",
        "Em redes neurais, trabalhamos com entradas que são multiplicadas por alguns pesos(w), somadas e enviadas para uma função de ativação para gerar um output. <br>\n",
        "É possível que entre o input (neurônios de entrada) e o input (neurônios de saída), haja diversas camadas múltiplas com neurônios escondidos. <br>\n",
        "Outros exemplos são as Redes Neurais Convulocionais (CNN), redes neurais recorrentes (RNN), rede de Kohonen e rede de Hopfield.<br>\n",
        "<img src=\"https://www.trabalhosescolares.net/wp-content/uploads/2007/05/redes_neurais_3_trabalhos_escolares.png\">\n",
        "\n",
        "EM SVMs, buscamos separar elementos (por ex. elementos azuis de vermelhos) através de uma função matemática que busca separar linearmente os dados (mudando a dimensão dos dados de 2d para 3d e buscando um plano que os separe, por exemplo). Sua diferença para as redes neurais se da na forma de como o hiperplano é calculado (distância entre os vetores de suporte e o hiperplano separador).<br>\n",
        "Ela pode ser usada para problemas de regressão e classificação.<br>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png\">\n",
        "\n",
        "---\n",
        "\n",
        "Aprendizagem não supervisionada - **Clusterização**: <br>\n",
        "Usada quando os dados de saída não estão disponíveis, e um exemplo bem comum de uso, é o de traçar perfis (com características semelhantes) de clientes em uma dada loja (agrupamento). Isso se dá através de medidas como a euclidiana ou de Manhatan até o centróide.\n",
        "\n",
        "É importante para esta aprendizagem, a normalização dos dados (colocar na mesma escala). Para isso, pode-se usar os métodos min-max, z-score, desvio absoluto médio etc.\n",
        "\n",
        "Podemos calcular em qual grupo o novo ponto estará (hard clustering) ou a probabilidade dele pertencer à um grupo (soft clustering).\n",
        "\n",
        "Exemplos de algoritmos são o K-Means e Hierarchical Clustering.\n",
        "\n",
        "---\n",
        "\n",
        "Um grupo de algoritmos de machine learning aplicados a um conjunto de dados recebe o nome de **Ensemble**\n",
        "<img src=\"https://www.jcchouinard.com/wp-content/uploads/2021/11/image-10.png\">\n",
        "\n",
        "Como este é um método custoso, podemos enviar amostras aleatórias do conjunto de dados para cada modelo.\n",
        "\n",
        "Ao final do processo, combina-se os múltiplos modelos.\n",
        "\n",
        "Quando utilizamos modelos que seguem o mesmo princípio (ex. todos árvores de decisão), chamamos a técnicas de Bootstrap Aggregation ou Bagging. O algoritmo Random Forest, por exemplo, é um conjunto de árvores, ou método ensemble.\n",
        "\n",
        "Quando utilizamos modelos que seguem o mesmo princípio, mas que tentam corrigir os erros do modelo anterior, temos a técnica de Boosting (ex. Stochastic Gradient Boosting).\n",
        "\n",
        "---\n",
        "\n",
        "**Redução de Dimensionalidade**:<br>\n",
        "\n",
        "Apesar da parte boa do BIG DATA, ele retorna para nós o problema da alta dimensionalidade (muitos atributos na base de dados).\n",
        "\n",
        "Podemos contornar esse problema com técnicas de redução de dimensionalidade. <br>\n",
        " - Através da extração de atributos, retiramos atributos da base de dados. Nesta técnica, podemos utilizar por exemplo, Principal Component Analysis (PCA), Multidimensional Scaling, FastMap etc.\n",
        " - Através da seleção de atributos, selecionando quais atributos devem permanecer. Nesta técnica, podemos utilizar por exemplo, algoritmos de Ap. Máq. (como árvores de decisão), cálculo de dimensão fractal, wrapper etc.\n",
        "  - Também podemos mesclar atributos, como no caso em que Cidade e Estado se tornam 'Cidade - Estado' (ex. Poços, MG se torna Poços - MG)\n",
        "\n",
        "Outras técnicas para a redução da dimensionalidade são:\n",
        "- **Low Variance Filter** (filtro de baixa variância)\n",
        "  - Calcula-se a variância dos dados, caso ela seja muito baixa, pode-se eliminar a feature.\n",
        "- **High Correlation Filter**\n",
        "  - Filtrar as variáveis com alta correlação (problema da multicolinearidade  -> ex. 2 features representando a mesma informação, como idade e ano de nascimento).\n",
        "- **Random Forests / Ensemble Trees**\n",
        "  - Utiliza-se as árvores de decisão que criam um modelo que entrega uma lista das variáveis mais relevantes ao modelo.\n",
        "- **Missing Values Ratio** (taxa de valores missing)\n",
        "  - Calcula-se a taxa de valores nulos em uma feature, para talvez eliminá-la caso haja muitos valores missing.\n",
        "- **Forward Feature Construction**\n",
        "  - Começa-se com 1 atributo e testa-se a acurácia de acordo com a adição de atributos e combinações.\n",
        "- **Backward Feature Elimination**\n",
        "  - Semelhante ao Forward Feature Construction, mas elimina as variáveis em cada etapa e testa a nova acurácia.\n",
        "\n",
        "**PCA**<br>\n",
        "Se trata de um algortimo de aprendizagem não supervisionada, que encontra os componentes que representam a informação nas variáveis, podendo ser utilizada para gerar índices e agrupamentos de indivíduos, por exemplo.\n",
        "\n",
        "<img src=\"https://ashutoshtripathicom.files.wordpress.com/2019/07/pca_title-1.jpg\">\n",
        "\n",
        "Neste exemplo, transformamos 3 variáveis em 2 componentes (que contém informações presentes nas 3 variáveis). Cada componente resultante é uma combinação de N atributos presentes no conjunto de dados, reduzindo-se a dimensionalidade, ou seja, diminuindo a quantidade de dados enquanto se reduz a perda de informação.\n",
        "\n",
        "* Obs: O PCA necessita de dados normalizados"
      ],
      "metadata": {
        "id": "pnLOqihvpof2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Features Engineering com Variáveis Categóricas </h2>\n",
        "\n",
        "---\n",
        "\n",
        "Precisamos de alternativas para representar as variáveis categóricas de uma forma numérica, sem mudar a informação contida\n",
        "\n",
        "Ex. utilizando R\n",
        "\n",
        "Lendo o arquivo em um dataset:\n",
        "```r\n",
        "dataset_bank <- read.table(\"bank/bank-full-csv\", sep = \";\", header = True)\n",
        "View(dataset_bank)\n",
        "```\n",
        "\n",
        "Criando uma nova coluna através da coluna 'marital' (tipo de união conjugal):\n",
        "1. Lendo a coluna 'marital' da tabela dataset_bank\n",
        "```r\n",
        "table(dataset_bank$marital)\n",
        "```\n",
        "2. Instalando os pacotes dplyr e ggplot2 para plot dos gráficos\n",
        "```r\n",
        "library(dplyr)\n",
        "library(ggplot2)\n",
        "dataset_bank %>%\n",
        "    group_by(marital)%>%\n",
        "    summarise(n = n())%>%\n",
        "    ggplot(aes(x = job, y = n))+\n",
        "    geom_bar(stat = \"identity\")+\n",
        "    theme(axis.text.x = element_text(angle = 90, hjust= 1))\n",
        "```\n",
        "\n",
        "3. Adicionando uma nova coluna traduzindo as uniões conjugais para português\n",
        "```r\n",
        "dataset_bank <- dataset_bank %>%\n",
        "    mutate(marital_pt_br =\n",
        "        case_when(marital = \"divorced\" ~ \"divorciado\",\n",
        "                  marital = \"married\" ~ \"casado\",\n",
        "                  (...)\n",
        "        )\n",
        "View(data_set_bank)\n",
        "```\n",
        "Obs: caso deixe algum valor sem correspondente, será inserido um valor 'n/a' no lugar.\n",
        "\n",
        "4. Agrupando as pessoas por tipo de união e arredondano a porcentagem em 2 casas\n",
        "```r\n",
        "round(prop.table(table(dataset_bank$marital_pt_br)),2)\n",
        "```\n",
        "\n",
        "--- \n",
        "\n",
        "Transformando uma variável categórica em um valor (variáveis Dummies).<br>\n",
        "\n",
        "5. Ex. Colocar 1 para pessoas casadas e 0 para as outras uniões criando uma nova coluna\n",
        "```r\n",
        "dataset_bank <- dataset_bank %>%\n",
        "    mutate(is_married = ifelse(marital = \"married\", 1, 0))\n",
        "```\n",
        "\n",
        "6. Podemos utilizar também a técnica de one-hot encoding (criamos colunas com os valores dos atributos, colocamos 1 para quando a variável apresenta essa característica e 0 quando para as outras). Por exemplo, caso haja 3 tipos de união conjugal, criaremos 3 colunas \"married\", \"divorced\" e \"single\", e caso a pessoa do registro seja casada, a coluna \"married\" recebe 1 e as outras duas colunas recebem 0.<br>\n",
        "Criando um novo dataframe com as variáveis categóricas em formato de one-hot encoding:\n",
        "```r\n",
        "library(caret)\n",
        "var_dummies <- dummyVars(\"~ .\", data = dataset_bank)\n",
        "bank.dummies <- data.frame(predict, var_dummies, newdata =  dataset_bank))\n",
        "View(bank.dummies)\n",
        "```\n",
        "\n",
        "Extra.<br>\n",
        "Agrupando dados em R (agrupando número de pessoas por tipo de união e grau de educação) \n",
        "```r\n",
        "dataset_bank %>%\n",
        "    group_by(marital, education) %>%\n",
        "    summarise(n = n())\n",
        "```\n",
        "\n",
        "Extra2. <br>\n",
        "Criando variáveis dummy que representam a junção (agrupamento) de educação e união conjugal\n",
        "```r\n",
        "var_dummies <- dummyvars( ~ marital:education, data = dataset_bank)\n",
        "bank.cross <- predict(var_dummies, newdata = dataset_bank)\n",
        "View(bank.cross_mar_and_edu)\n",
        "```\n"
      ],
      "metadata": {
        "id": "Z-Pa_qkAug-A"
      }
    }
  ]
}