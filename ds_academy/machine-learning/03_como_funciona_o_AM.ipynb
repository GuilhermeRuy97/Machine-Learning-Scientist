{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A ideia do aprendizado de máquina é a de dado um input e output, descobrir qual (uma ou mais) a **função matemática** responsável por tornar o input no output.\n",
        "\n",
        "Dado um input, realizamos a **feature extraction** (que o deep learning faz sozinho, porém requer mais dados) para então classificar e gerar um output.\n",
        "\n",
        "Ou seja, é importantíssimo para os modelos de machine learning serem capazes de **generalizar**. Caso ele não consiga realizar essa generalização, ele apenas estará \"decorando\" os dados. Ele deverá fazer previsões em dados que ele nunca havia visto antes.\n",
        "\n",
        "Durante o processo, o engine de otimização altera os valores iniciais dos parâmetros para representar a **função alvo**. Durante a fase de otimização os algoritmos buscam as variações e combinações entre os parâmetros. Essas funções se situam no **espaço de hipóteses** que contém as variações dos parâmetros em um algoritmo de ML, e conterá ao final, a função alvo que resolverá o problema.\n",
        "\n",
        "O conjunto de dados também precisa ter um padrão e, caso ele não exista, o aprendizado de máquina não será útil.\n",
        "\n",
        "---\n",
        "**Componentes do processo de aprendizagem**:<br>\n",
        "- Input       ~> X<br>\n",
        "- Output      ~> y<br>\n",
        "- Função alvo ~> f: x -> y<br>\n",
        "- Dados       ~> (X1, y1), ..., (Xn, yn)<br>\n",
        "- Hipótese    ~> g: x -> y\n",
        "\n",
        "Obs: Como não conseguimos encontrar a função f (que representa a relação entre X e y) no mundo real, encontraremos a função g que é **aproximada de f**.\n",
        "- O espaço de hipóteses é onde buscamos a melhor função alvo.\n",
        "\n",
        "Espaço de hipótese:<br>\n",
        "H = {h} , onde:<br>\n",
        "- H = espaço de hipóteses<br>\n",
        "- h = conjunto de hipóteses<br>\n",
        "Queremos então que:<br>\n",
        "- g∈H (g faça parte do espaço de hipóteses), ou seja:<br>\n",
        "- **Espaço de hipóteses + Algoritmo = Modelo**\n",
        "\n",
        ">No exemplo de redes neurais, elas representam o espaço de hipóteses, e o algoritmo, por exemplo, o feedforward (o backpropagation é muito bom para isso).\n",
        "\n",
        ">No exemplo da pessoa poder ou não estar doente, os inputs são multiplicados por **pesos** 'w' e caso a somatória seja maior que o **treshold**, a pessoa será decretada doente, caso seja menor que o **treshold** ela será considera sadia. A comparação com o treshold em redes neurais é feito pela **função de ativação**.\n",
        "\n",
        "\n",
        "Ou seja, o algoritmo de aprendizagem é dado por:<br>\n",
        "`h(X) = sign(W^tX)`<br>\n",
        "=> As diferentes combinações de peso e treshold formarão diferentes hipóteses. Cabe a nós encontrarmos a melhor hipótese (melhor modelo possível), ou melhor aproximação da função alvo.\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20191014210129/pic82-e1571070331775.png\">\n",
        "Neste exemplo, onde os dados são **linearmente separáveis**, cada reta vermelha é uma hipótese. Devemos então, ver qual hipótese melhor separa os dados.\n",
        "\n",
        "E quando acontece um erro de classificação?<br>\n",
        "`sign(W^tX) != yn`<br>\n",
        "=> Ajustaremos o processo de aprendizagem dado este erro, alterando os pesos durante as iterações.<br>\n",
        "`w <- w + ynXn`\n",
        "\n",
        "---\n",
        "**Cost Function**: (ie. loss function) <br>\n",
        "Descreve o quão bem a resposta no espaço de hipóteses se encaixa no conjunto de dados analisados.\n",
        "\n",
        "A função pode ser dada por:<br>\n",
        "`J(yi, h(Xi))`, onde: <br>\n",
        "y => valores observados <br>\n",
        "h(x) => valores previstos encontrados pelo modelo <br>\n",
        "J => número que melhor representa a relação entre os valores observados e os preditos. (Diferença entre o valor esperado e o valor predito).\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1400/1*tQTcGTLZqnI5rp3JYO_4NA.png\">\n",
        "\n",
        "Ou seja, a cada iteração, usamos a cost function para atualizar os pesos, para que h(X) chegue mais próximo de y.\n",
        "\n",
        "Como saber se estamos encontrando a melhor solução?\n",
        "\n",
        "--- \n",
        "**Gradient Descent**: <br>\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/772/1*wfEkrwouTeHv76SX5x7JqA.jpeg\">\n",
        "\n",
        "Repare que temos **mínimos locais** e **mínimo global**, também podemos ter locais \"platô\", que é quando o algoritmo fica \"preso\" em algum local.\n",
        "\n",
        "Exemplo utilizando Python para encontrar os mínimos locais da função y = (x+6)^2, iniciando do parâmetro x = 2: <br>\n",
        "```python\n",
        "# iniciando o parâmetro em 2\n",
        "x_current = 2\n",
        "# Learning rate (tamanho do passo na busca)\n",
        "rate = 0.01\n",
        "# Parar o algoritmo quando a diferença entre 2 iterações for menor que 0.0001 ou passar de 5.000 iterações\n",
        "precision = 0.0001\n",
        "max_iters = 5000\n",
        "# contador de passos anterior\n",
        "previous_step_size = 1\n",
        "# Contador de iterações\n",
        "count_iters = 0\n",
        "# Gradiente da função (que obtido pela derivada)\n",
        "dataframe = lambda x: 2 * (x+6)\n",
        "\n",
        "while previous_step_size > precision and iters < max_iters:\n",
        "  # Armazena o valor atual de x em prev_x\n",
        "  prev_x = x_current\n",
        "  # Aplicando o gradient descent ao dataframe com o resultado do cálculo do gradiente da função\n",
        "  x_current = x_current - rate * dataframe(prev_x)\n",
        "  # Alterando x\n",
        "  previous_step_size = abs(x_current - prev_x)\n",
        "\n",
        "  print(f\"Iteração: {iters}\\n X = {x_current}\")\n",
        "\n",
        "print(f'O mínimo local acontece em {x_current}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Urderfitting** e **Overfitting** acontecem quando não conseguimos generalizar bem (resposta do modelo frente a dados novos) o modelo.\n",
        "\n",
        "- Quando temos um underfit, temos um grande viés (bias)\n",
        "- Quando temos um overfit, temos uma grande variância.\n",
        "\n",
        "É muito comum acontecer o overfitting em redes neurais. Portanto, precisamos realizar algumas técnicas para diminuir isso, como a Poda (pruning em árvores de decisão), regularização etc.\n",
        "\n",
        "No caso de underfitting, podemos ter dados insuficientes ou um algoritmo não ideal para o caso.\n",
        "\n",
        "Um good fit se da em geral numa faixa de taxa de acerto de 80% a 98%, uma taxa de acerto de 100% pode representar um problema.\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/aVxfY.png\">"
      ],
      "metadata": {
        "id": "u8ChcIvQ6BiP"
      }
    }
  ]
}